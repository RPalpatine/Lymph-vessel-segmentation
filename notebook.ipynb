{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils2 import DataLoaderSegmentation\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import segmentation_models_pytorch as smp\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import albumentations as albu\n",
    "import torch.utils.data as data_utils\n",
    "from segmentation_models_pytorch import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"C:/Users/rasic/OpenCV/DL_WITH_KERAS/QuPath - Copy for DL/tiles/images/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"E:/Niklas_projekt/LVSegmentationCode/dataset/train/\"\n",
    "VALID_PATH = \"E:/Niklas_projekt/LVSegmentationCode/dataset/valid/\"\n",
    "TEST_PATH = \"E:/Niklas_projekt/LVSegmentationCode/dataset/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"E:/Niklas_projekt/LVSegmentationCode/dataset/train/\"\n",
    "VALID_PATH = \"E:/Niklas_projekt/LVSegmentationCode/dataset/valid/\"\n",
    "TEST_PATH = \"E:/Niklas_projekt/LVSegmentationCode/dataset/test/\"\n",
    "\n",
    "train_image_files = [TRAIN_PATH + \"/images/\" + file for file in os.listdir(TRAIN_PATH + \"/images/\")]\n",
    "train_mask_files = [TRAIN_PATH + \"/masks/\" + file for file in os.listdir(TRAIN_PATH + \"/masks/\")]\n",
    "\n",
    "valid_image_files = [VALID_PATH + \"/images/\" + file for file in os.listdir(VALID_PATH + \"/images/\")]\n",
    "valid_mask_files = [VALID_PATH + \"/masks/\" + file for file in os.listdir(VALID_PATH + \"/masks/\")]\n",
    "\n",
    "test_image_files = [TEST_PATH + \"/images/\" + file for file in os.listdir(TEST_PATH + \"/images/\")]\n",
    "test_mask_files = [TEST_PATH + \"/masks/\" + file for file in os.listdir(TEST_PATH + \"/masks/\")]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.DeepLabV3(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "         # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(\"resnet34\", \"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "functools.partial(<function preprocess_input at 0x0000021E7FE060E0>, input_space='RGB', input_range=[0, 1], mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = transforms.Compose([transforms.RandomRotation(degrees=(20)),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations_mask = transforms.Compose([transforms.RandomRotation(degrees=(20)),\n",
    "                                    transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_to_tensor_and_normalize = transforms.Compose([transforms.ToTensor()])\n",
    "transforms_to_tensor = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DataLoaderSegmentation(img_files=train_image_files, mask_files=train_mask_files, transforms_img=augmentations, transforms_mask=augmentations)\n",
    "valid_dataset = DataLoaderSegmentation(img_files=valid_image_files, mask_files=valid_mask_files, transforms_img=transform_to_tensor_and_normalize, transforms_mask=transforms_to_tensor)\n",
    "test_dataset = DataLoaderSegmentation(img_files=test_image_files, mask_files=test_mask_files, transforms_img=transform_to_tensor_and_normalize, transforms_mask=transforms_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 128, 128])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader =  data_utils.DataLoader(train_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "valid_loader = data_utils.DataLoader(valid_dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet34\",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "    encoder_weights=\"imagenet\",     # use `imagenet` pre-trained weights for encoder initialization\n",
    "    in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "    classes=1,                      # model output channels (number of classes in your dataset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = smp.utils.losses.JaccardLoss()\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(),\n",
    "]\n",
    "\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.01),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=\"cuda\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=\"cuda\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train: 100%|██████████| 29/29 [00:25<00:00,  1.13it/s, jaccard_loss - -14.33, iou_score - 0.6736]\n",
      "valid: 100%|██████████| 4/4 [00:03<00:00,  1.12it/s, jaccard_loss - -5.015, iou_score - 0.1552]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 1\n",
      "train: 100%|██████████| 29/29 [00:15<00:00,  1.83it/s, jaccard_loss - 173.0, iou_score - 0.2873] \n",
      "valid: 100%|██████████| 4/4 [00:03<00:00,  1.23it/s, jaccard_loss - -95.87, iou_score - 0.003041] \n",
      "\n",
      "Epoch: 2\n",
      "train: 100%|██████████| 29/29 [00:15<00:00,  1.89it/s, jaccard_loss - -6.679, iou_score - 0.1804]\n",
      "valid: 100%|██████████| 4/4 [00:02<00:00,  1.36it/s, jaccard_loss - -1.103, iou_score - 0.1331]\n",
      "\n",
      "Epoch: 3\n",
      "train: 100%|██████████| 29/29 [00:14<00:00,  1.95it/s, jaccard_loss - -6.982, iou_score - 0.2176]\n",
      "valid: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s, jaccard_loss - -17.13, iou_score - 0.3873]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 4\n",
      "train: 100%|██████████| 29/29 [00:14<00:00,  1.93it/s, jaccard_loss - -8.338, iou_score - 0.2569]\n",
      "valid: 100%|██████████| 4/4 [00:03<00:00,  1.32it/s, jaccard_loss - -27.67, iou_score - 0.6224]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 5\n",
      "train: 100%|██████████| 29/29 [00:15<00:00,  1.90it/s, jaccard_loss - -10.14, iou_score - 0.3076]\n",
      "valid: 100%|██████████| 4/4 [00:03<00:00,  1.27it/s, jaccard_loss - -30.55, iou_score - 0.6511]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 6\n",
      "train: 100%|██████████| 29/29 [00:15<00:00,  1.91it/s, jaccard_loss - -11.8, iou_score - 0.3368] \n",
      "valid: 100%|██████████| 4/4 [00:02<00:00,  1.34it/s, jaccard_loss - -46.78, iou_score - 0.6182]\n",
      "\n",
      "Epoch: 7\n",
      "train: 100%|██████████| 29/29 [00:15<00:00,  1.90it/s, jaccard_loss - -15.84, iou_score - 0.3728]\n",
      "valid: 100%|██████████| 4/4 [00:03<00:00,  1.30it/s, jaccard_loss - -26.19, iou_score - 0.9042]\n",
      "Model saved!\n",
      "\n",
      "Epoch: 8\n",
      "train:   0%|          | 0/29 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m40\u001b[39m):\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mEpoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i))\n\u001b[1;32m----> 6\u001b[0m     train_logs \u001b[39m=\u001b[39m train_epoch\u001b[39m.\u001b[39;49mrun(train_loader)\n\u001b[0;32m      7\u001b[0m     valid_logs \u001b[39m=\u001b[39m valid_epoch\u001b[39m.\u001b[39mrun(valid_loader)\n\u001b[0;32m      9\u001b[0m     \u001b[39m# do something (save model, change lr, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rasic\\anaconda3\\envs\\opencv-course\\lib\\site-packages\\segmentation_models_pytorch\\utils\\train.py:49\u001b[0m, in \u001b[0;36mEpoch.run\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     41\u001b[0m metrics_meters \u001b[39m=\u001b[39m {metric\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m: AverageValueMeter() \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics}\n\u001b[0;32m     43\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(\n\u001b[0;32m     44\u001b[0m     dataloader,\n\u001b[0;32m     45\u001b[0m     desc\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstage_name,\n\u001b[0;32m     46\u001b[0m     file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstdout,\n\u001b[0;32m     47\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose),\n\u001b[0;32m     48\u001b[0m ) \u001b[39mas\u001b[39;00m iterator:\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m iterator:\n\u001b[0;32m     50\u001b[0m         x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice), y\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     51\u001b[0m         loss, y_pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_update(x, y)\n",
      "File \u001b[1;32mc:\\Users\\rasic\\anaconda3\\envs\\opencv-course\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rasic\\anaconda3\\envs\\opencv-course\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:435\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[0;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 435\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[1;32mc:\\Users\\rasic\\anaconda3\\envs\\opencv-course\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:381\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    380\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 381\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\rasic\\anaconda3\\envs\\opencv-course\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1034\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1027\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1030\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1034\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[0;32m   1035\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1036\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\rasic\\anaconda3\\envs\\opencv-course\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rasic\\anaconda3\\envs\\opencv-course\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\rasic\\anaconda3\\envs\\opencv-course\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_win32\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[1;32mc:\\Users\\rasic\\anaconda3\\envs\\opencv-course\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[39m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     reduction\u001b[39m.\u001b[39;49mdump(process_obj, to_child)\n\u001b[0;32m     94\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\rasic\\anaconda3\\envs\\opencv-course\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(obj, file, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m     \u001b[39m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     ForkingPickler(file, protocol)\u001b[39m.\u001b[39;49mdump(obj)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_score = 0\n",
    "\n",
    "for i in range(0, 40):\n",
    "    \n",
    "    print('\\nEpoch: {}'.format(i))\n",
    "    train_logs = train_epoch.run(train_loader)\n",
    "    valid_logs = valid_epoch.run(valid_loader)\n",
    "    \n",
    "    # do something (save model, change lr, etc.)\n",
    "    if max_score < valid_logs['iou_score']:\n",
    "        max_score = valid_logs['iou_score']\n",
    "        torch.save(model, './best_model.pth')\n",
    "        print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
